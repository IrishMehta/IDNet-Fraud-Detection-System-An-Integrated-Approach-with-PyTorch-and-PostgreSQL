{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8825926-9aa4-4b00-94af-266258397c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60036c8a-3ee4-4ceb-bb37-3e49b38b4601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ihmehta/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ihmehta/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Set up transformations for the images\n",
    "\n",
    "train_transform= transforms.Compose([\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to fit model input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Standard ImageNet normalization\n",
    "])\n",
    "\n",
    "\n",
    "# Step 2: Load the dataset using ImageFolder\n",
    "train_dataset = ImageFolder(root=\"../dataset/train\", transform=train_transform)\n",
    "test_dataset = ImageFolder(root=\"../dataset/test\", transform=test_transform)\n",
    "out_of_sample_dataset = ImageFolder(root=\"../dataset/out_of_sample_test\", transform=test_transform)\n",
    "\n",
    "# Creating vaidation set\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for batching and shuffling\n",
    "train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "out_of_sample_loader = DataLoader(out_of_sample_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Create your custom CNN model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 3)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b68d8c8-092d-4487-be41-78c697929dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ihmehta/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation accuracy: 0.6190\n",
      "Epoch [1/40], Loss: 1.0868, Train Accuracy: 0.5352, Validation Accuracy: 0.6190\n",
      "New best model saved with validation accuracy: 0.7143\n",
      "Epoch [2/40], Loss: 1.0596, Train Accuracy: 0.6348, Validation Accuracy: 0.7143\n",
      "Epoch [3/40], Loss: 1.0307, Train Accuracy: 0.6268, Validation Accuracy: 0.6310\n",
      "Epoch [4/40], Loss: 1.0114, Train Accuracy: 0.5179, Validation Accuracy: 0.5238\n",
      "Epoch [5/40], Loss: 0.9392, Train Accuracy: 0.6149, Validation Accuracy: 0.6786\n",
      "Epoch [6/40], Loss: 0.8544, Train Accuracy: 0.7211, Validation Accuracy: 0.6429\n",
      "New best model saved with validation accuracy: 0.8214\n",
      "Epoch [7/40], Loss: 0.7864, Train Accuracy: 0.7384, Validation Accuracy: 0.8214\n",
      "New best model saved with validation accuracy: 0.8810\n",
      "Epoch [8/40], Loss: 0.5945, Train Accuracy: 0.8526, Validation Accuracy: 0.8810\n",
      "Epoch [9/40], Loss: 0.5112, Train Accuracy: 0.8911, Validation Accuracy: 0.8095\n",
      "Epoch [10/40], Loss: 0.4156, Train Accuracy: 0.9163, Validation Accuracy: 0.8690\n",
      "New best model saved with validation accuracy: 0.9048\n",
      "Epoch [11/40], Loss: 0.3773, Train Accuracy: 0.9416, Validation Accuracy: 0.9048\n",
      "New best model saved with validation accuracy: 0.9167\n",
      "Epoch [12/40], Loss: 0.3066, Train Accuracy: 0.9416, Validation Accuracy: 0.9167\n",
      "Epoch [13/40], Loss: 0.2812, Train Accuracy: 0.9615, Validation Accuracy: 0.9048\n",
      "New best model saved with validation accuracy: 0.9286\n",
      "Epoch [14/40], Loss: 0.2641, Train Accuracy: 0.9535, Validation Accuracy: 0.9286\n",
      "Epoch [15/40], Loss: 0.1952, Train Accuracy: 0.9734, Validation Accuracy: 0.9167\n",
      "Epoch [16/40], Loss: 0.2048, Train Accuracy: 0.9761, Validation Accuracy: 0.9167\n",
      "Epoch [17/40], Loss: 0.1509, Train Accuracy: 0.9814, Validation Accuracy: 0.8929\n",
      "Epoch [18/40], Loss: 0.1585, Train Accuracy: 0.9708, Validation Accuracy: 0.9286\n",
      "New best model saved with validation accuracy: 0.9524\n",
      "Epoch [19/40], Loss: 0.0970, Train Accuracy: 0.9894, Validation Accuracy: 0.9524\n",
      "Epoch [20/40], Loss: 0.2188, Train Accuracy: 0.9814, Validation Accuracy: 0.9524\n",
      "Epoch [21/40], Loss: 0.1840, Train Accuracy: 0.9774, Validation Accuracy: 0.9405\n",
      "Epoch [22/40], Loss: 0.0947, Train Accuracy: 0.9880, Validation Accuracy: 0.9286\n",
      "Epoch [23/40], Loss: 0.0897, Train Accuracy: 0.9854, Validation Accuracy: 0.9167\n",
      "New best model saved with validation accuracy: 0.9643\n",
      "Epoch [24/40], Loss: 0.1245, Train Accuracy: 0.9774, Validation Accuracy: 0.9643\n",
      "Epoch [25/40], Loss: 0.1555, Train Accuracy: 0.9894, Validation Accuracy: 0.9167\n",
      "Epoch [26/40], Loss: 0.1212, Train Accuracy: 0.9841, Validation Accuracy: 0.9405\n",
      "Epoch [27/40], Loss: 0.1287, Train Accuracy: 0.9814, Validation Accuracy: 0.9286\n",
      "Epoch [28/40], Loss: 0.1129, Train Accuracy: 0.9867, Validation Accuracy: 0.9048\n",
      "Epoch [29/40], Loss: 0.1745, Train Accuracy: 0.9880, Validation Accuracy: 0.8929\n",
      "Epoch [30/40], Loss: 0.1564, Train Accuracy: 0.9867, Validation Accuracy: 0.9167\n",
      "Epoch [31/40], Loss: 0.0598, Train Accuracy: 0.9920, Validation Accuracy: 0.9048\n",
      "Epoch [32/40], Loss: 0.0486, Train Accuracy: 0.9960, Validation Accuracy: 0.9167\n",
      "Epoch [33/40], Loss: 0.0534, Train Accuracy: 0.9920, Validation Accuracy: 0.9167\n",
      "Epoch [34/40], Loss: 0.0537, Train Accuracy: 0.9920, Validation Accuracy: 0.9286\n",
      "Epoch [35/40], Loss: 0.1070, Train Accuracy: 0.9960, Validation Accuracy: 0.9286\n",
      "Epoch [36/40], Loss: 0.0668, Train Accuracy: 0.9894, Validation Accuracy: 0.9048\n",
      "Epoch [37/40], Loss: 0.0509, Train Accuracy: 0.9973, Validation Accuracy: 0.8929\n",
      "Epoch [38/40], Loss: 0.0509, Train Accuracy: 0.9947, Validation Accuracy: 0.9286\n",
      "Epoch [39/40], Loss: 0.0450, Train Accuracy: 0.9973, Validation Accuracy: 0.9167\n",
      "Epoch [40/40], Loss: 0.0608, Train Accuracy: 0.9934, Validation Accuracy: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1898438/3568260059.py:163: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_fraud_detection_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL MODEL EVALUATION\n",
      "==================================================\n",
      "\n",
      "Training Set Performance:\n",
      "Overall Accuracy: 0.9947\n",
      "Per-class Accuracy:\n",
      "  Class 0: 1.0000\n",
      "  Class 1: 1.0000\n",
      "  Class 2: 0.9487\n",
      "\n",
      "Test Set Performance:\n",
      "Overall Accuracy: 0.9016\n",
      "Per-class Accuracy:\n",
      "  Class 0: 0.9592\n",
      "  Class 1: 0.8333\n",
      "  Class 2: 0.5000\n",
      "\n",
      "Out-of-Sample Set Performance:\n",
      "Overall Accuracy: 0.8800\n",
      "Per-class Accuracy:\n",
      "  Class 0: 0.9500\n",
      "  Class 1: 1.0000\n",
      "  Class 2: 0.2000\n",
      "\n",
      "Comparison of Metrics Across Datasets:\n",
      "      Dataset  Overall Accuracy  Class 0 Accuracy  Class 1 Accuracy  Class 2 Accuracy  Class 0 F1-Score  Class 1 F1-Score  Class 2 F1-Score\n",
      "     Training          0.994688          1.000000          1.000000          0.948718          1.000000          0.973684          0.973684\n",
      "         Test          0.901639          0.959184          0.833333          0.500000          0.979167          0.625000          0.600000\n",
      "Out-of-Sample          0.880000          0.950000          1.000000          0.200000          0.974359          0.625000          0.333333\n",
      "\n",
      "Metrics saved to 'fraud_detection_metrics.csv'\n",
      "Final model saved as fraud_detection_model.pth\n",
      "\n",
      "Model training and evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define the loss function and optimizer\n",
    "# Calculate class weights inverse to their frequency\n",
    "\n",
    "# Get all training labels\n",
    "all_labels = []\n",
    "for _, labels in train_loader:\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.2526746729247135e-05, weight_decay=0.0007394206084128669)\n",
    "\n",
    "# Create a scheduler to reduce learning rate when validation loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.13777011736928238, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# Initialize lists to track metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "# Step 5: Train the model with validation after each epoch\n",
    "num_epochs = 40\n",
    "\n",
    "def evaluate_model(model, data_loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with detailed metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    class_correct = [0] * len(class_names)\n",
    "    class_total = [0] * len(class_names)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            \n",
    "            # For per-class accuracy\n",
    "            for i in range(len(preds)):\n",
    "                label = labels[i].item()\n",
    "                class_correct[label] += (preds[i] == labels[i]).item()\n",
    "                class_total[label] += 1\n",
    "            \n",
    "            # For confusion matrix and classification report\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    per_class_accuracy = {}\n",
    "    for i in range(len(class_names)):\n",
    "        if class_total[i] > 0:\n",
    "            per_class_accuracy[class_names[i]] = class_correct[i] / class_total[i]\n",
    "        else:\n",
    "            per_class_accuracy[class_names[i]] = 0\n",
    "    \n",
    "    # Generate confusion matrix and classification report\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True,  zero_division=0)\n",
    "    \n",
    "    return accuracy, per_class_accuracy, conf_matrix, class_report, all_labels, all_preds\n",
    "\n",
    "\n",
    "# Get class names from your dataset\n",
    "class_names = train_dataset.classes  # Should be ['0', '1', '2']\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track the loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_preds += (preds == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "    \n",
    "    # Calculate epoch accuracy and loss\n",
    "    epoch_accuracy = correct_preds / total_preds\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "    \n",
    "    # Evaluation on validation set (test set in this case)\n",
    "    val_accuracy, val_per_class_acc, _, _, _, _ = evaluate_model(model, val_loader, device, class_names)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # Update learning rate based on validation performance\n",
    "    scheduler.step(epoch_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), \"best_fraud_detection_model.pth\")\n",
    "        print(f\"New best model saved with validation accuracy: {best_val_accuracy:.4f}\")\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Loss: {epoch_loss:.4f}, \"\n",
    "          f\"Train Accuracy: {epoch_accuracy:.4f}, \"\n",
    "          f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Print per-class accuracy on validation set\n",
    "    # print(\"Test Per-class Accuracy:\")\n",
    "    # for cls_name, acc in val_per_class_acc.items():\n",
    "    #     print(f\"  Class {cls_name}: {acc:.4f}\")\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train')\n",
    "plt.plot(val_accuracies, label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_progress.png')\n",
    "plt.close()\n",
    "\n",
    "# Load the best model for final evaluation\n",
    "model.load_state_dict(torch.load(\"best_fraud_detection_model.pth\"))\n",
    "\n",
    "# Final evaluation on all datasets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Evaluate on training set\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "train_acc, train_per_class_acc, train_cm, train_report, _, _ = evaluate_model(\n",
    "    model, train_loader, device, class_names\n",
    ")\n",
    "print(f\"Overall Accuracy: {train_acc:.4f}\")\n",
    "print(\"Per-class Accuracy:\")\n",
    "for cls_name, acc in train_per_class_acc.items():\n",
    "    print(f\"  Class {cls_name}: {acc:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nTest Set Performance:\")\n",
    "test_acc, test_per_class_acc, test_cm, test_report, _, _ = evaluate_model(\n",
    "    model, test_loader, device, class_names\n",
    ")\n",
    "print(f\"Overall Accuracy: {test_acc:.4f}\")\n",
    "print(\"Per-class Accuracy:\")\n",
    "for cls_name, acc in test_per_class_acc.items():\n",
    "    print(f\"  Class {cls_name}: {acc:.4f}\")\n",
    "\n",
    "# Evaluate on out-of-sample set\n",
    "print(\"\\nOut-of-Sample Set Performance:\")\n",
    "oos_acc, oos_per_class_acc, oos_cm, oos_report, _, _ = evaluate_model(\n",
    "    model, out_of_sample_loader, device, class_names\n",
    ")\n",
    "print(f\"Overall Accuracy: {oos_acc:.4f}\")\n",
    "print(\"Per-class Accuracy:\")\n",
    "for cls_name, acc in oos_per_class_acc.items():\n",
    "    print(f\"  Class {cls_name}: {acc:.4f}\")\n",
    "\n",
    "# Create a DataFrame for all metrics to easily compare\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Dataset': ['Training', 'Test', 'Out-of-Sample'],\n",
    "    'Overall Accuracy': [train_acc, test_acc, oos_acc],\n",
    "    'Class 0 Accuracy': [train_per_class_acc.get('0', 0), test_per_class_acc.get('0', 0), oos_per_class_acc.get('0', 0)],\n",
    "    'Class 1 Accuracy': [train_per_class_acc.get('1', 0), test_per_class_acc.get('1', 0), oos_per_class_acc.get('1', 0)],\n",
    "    'Class 2 Accuracy': [train_per_class_acc.get('2', 0), test_per_class_acc.get('2', 0), oos_per_class_acc.get('2', 0)],\n",
    "    'Class 0 F1-Score': [train_report['0']['f1-score'], test_report['0']['f1-score'], oos_report['0']['f1-score']],\n",
    "    'Class 1 F1-Score': [train_report['1']['f1-score'], test_report['1']['f1-score'], oos_report['1']['f1-score']],\n",
    "    'Class 2 F1-Score': [train_report['2']['f1-score'], test_report['2']['f1-score'], oos_report['2']['f1-score']]\n",
    "})\n",
    "\n",
    "# Display the metrics table\n",
    "print(\"\\nComparison of Metrics Across Datasets:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_df.to_csv('fraud_detection_metrics.csv', index=False)\n",
    "print(\"\\nMetrics saved to 'fraud_detection_metrics.csv'\")\n",
    "\n",
    "# Step 7: Save the final model\n",
    "torch.save(model.state_dict(), \"fraud_detection_model.pth\")\n",
    "print(\"Final model saved as fraud_detection_model.pth\")\n",
    "\n",
    "print(\"\\nModel training and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55f117-84d6-4964-85e5-4151bb9986fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74873202-2750-44d5-b475-1e3a7254581d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1eb2b-46b6-46fa-b8b3-37fba98a8f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
